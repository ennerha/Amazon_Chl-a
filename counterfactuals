# ============================================================
# Counterfactuals — TOP-K (Amazônia/Drive)
# - Integra com DRIVE_ROOT e artefatos do seu pipeline
# - Usa svr_pipe/X_all/FEATURES já no notebook OU carrega do Drive
# - Multi-restart paralelo + diversidade + plots/CSV em DRIVE_ROOT/xai_outputs/cf
# ============================================================

import os, math, json
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from joblib import Parallel, delayed

# -------------------- Drive & Artefatos --------------------
DRIVE_ROOT = Path(os.environ.get("DRIVE_ROOT", "/content/drive/MyDrive/amazonia_modis_artifacts"))
IN_MODELS  = DRIVE_ROOT / "inputs" / "models"
OUT_DIR    = DRIVE_ROOT / "xai_outputs" / "cf"
OUT_DIR.mkdir(parents=True, exist_ok=True)

MODEL_PATH        = IN_MODELS / "amazonia_svr_global.pkl"   # gerado pelo treinamento
FALLBACK_XALL_NPY = IN_MODELS / "X_all.npy"                 # opcional (salve do treino)
FALLBACK_FEATS_JSON = IN_MODELS / "features.json"           # opcional (salve do treino)

print(f"[INFO] Drive root: {DRIVE_ROOT}")
print(f"[INFO] Modelo:     {MODEL_PATH}")
print(f"[INFO] Out dir:    {OUT_DIR}")

# -------------------- Carregar svr_pipe / X_all / FEATURES --------------------
_need_model = False
try:
    svr_pipe    # type: ignore
    FEATURES    # type: ignore
    X_all       # type: ignore
    print("[OK] Usando objetos já carregados no notebook (svr_pipe/FEATURES/X_all).")
except NameError:
    _need_model = True

if _need_model:
    import joblib
    if not MODEL_PATH.exists():
        raise FileNotFoundError(f"Modelo não encontrado: {MODEL_PATH}")
    M = joblib.load(MODEL_PATH)
    svr_pipe = M["model"]
    FEATURES = list(M["features"])
    print(f"[OK] Modelo carregado. #features = {len(FEATURES)}")

    # Tenta carregar X_all/FEATURES de arquivos auxiliares (quantis/MAD precisam de distribuição)
    if 'X_all' not in globals():
        if FALLBACK_XALL_NPY.exists():
            X_all = np.load(FALLBACK_XALL_NPY)
            print(f"[OK] X_all carregado: {FALLBACK_XALL_NPY} | shape={X_all.shape}")
        else:
            # Último recurso: sintetiza limites a partir de uma amostra Gaussiana centrada em 0
            # (permite rodar, mas recomendo salvar X_all.npy do treino para limites realistas)
            print("[WARN] X_all.npy não encontrado — criando distribuição sintética p/ quantis/MAD.")
            d = len(FEATURES)
            rng_tmp = np.random.default_rng(123)
            X_all = rng_tmp.normal(0, 1, size=(2000, d)).astype(np.float32)

    # Confere (opcional) nomes de features
    if FALLBACK_FEATS_JSON.exists():
        try:
            feats_json = json.loads(FALLBACK_FEATS_JSON.read_text())
            if isinstance(feats_json, list) and len(feats_json) == len(FEATURES):
                # ok — só confirma consistência
                pass
        except Exception:
            pass

# -------------------- Config de Alvo/Instâncias --------------------
# Defina UM dos dois:
target_mgm3  = 30.0     # alvo em mg m^-3 (µg/L)
target_log1p = None     # ou diretamente em log1p

# Como selecionar as instâncias (índices em X_all)
instances_idx = [0, 1, 2]  # ou defina uma amostra aleatória
# Ex.: amostra aleatória estável
# rng_sel = np.random.default_rng(42)
# instances_idx = rng_sel.choice(X_all.shape[0], size=5, replace=False).tolist()

# Recursos imutáveis (por nome OU índice)
IMMUTABLE_FEATURES = []   # ex.: ["latitude","longitude"] ou [0, 5]

# -------------------- Hiperparâmetros do buscador --------------------
LOW_Q, HIGH_Q = 0.01, 0.99          # limites do suporte (quantis)
LAMBDA_DIST   = 0.25                # peso da distância L1/MAD
MAX_ITERS     = 800
COOLING       = 0.997
INIT_STEP     = 0.30
MIN_STEP      = 1e-4
TOL_Y         = 1e-3
NO_IMPROVE_K  = 60
SENS_K        = 12
EPS_GRAD      = 1e-3

N_RESTARTS       = 12
TOP_K_SOLUTIONS  = 5
DIVERSITY_MIN_L2 = 0.65

RANDOM_SEED   = 42
MAKE_PLOTS    = True
PLOT_DPI      = 200
TOP_N_CHANGES = 12
N_JOBS_RESTARTS = -1  # usa todos os núcleos

rng = np.random.default_rng(RANDOM_SEED)

# -------------------- Preparos: limites e escalas --------------------
X_low  = np.quantile(X_all, LOW_Q, axis=0).astype(np.float32)
X_high = np.quantile(X_all, HIGH_Q, axis=0).astype(np.float32)

mad = np.median(np.abs(X_all - np.median(X_all, axis=0)), axis=0).astype(np.float32)
if np.any(mad == 0):
    repl = np.median(mad[mad > 0]) if np.any(mad > 0) else 1.0
    mad[mad == 0] = np.float32(repl)

# Índices imutáveis
if len(IMMUTABLE_FEATURES) and isinstance(IMMUTABLE_FEATURES[0], str):
    name_to_idx = {n: i for i, n in enumerate(FEATURES)}
    immutable_idx = {name_to_idx[n] for n in IMMUTABLE_FEATURES if n in name_to_idx}
else:
    immutable_idx = set(int(i) for i in IMMUTABLE_FEATURES)

# -------------------- Helpers --------------------
def ensure_target_log1p(t_mgm3=None, t_log1p=None):
    if t_log1p is not None:
        return [float(t_log1p)] if np.isscalar(t_log1p) else [float(t) for t in t_log1p]
    if t_mgm3 is not None:
        return [float(np.log1p(t_mgm3))] if np.isscalar(t_mgm3) else [float(np.log1p(t)) for t in t_mgm3]
    raise ValueError("Defina target_mgm3 OU target_log1p.")

y_targets = ensure_target_log1p(target_mgm3, target_log1p)

def predict_batch(X_batch: np.ndarray) -> np.ndarray:
    Xb = X_batch.reshape(1, -1) if X_batch.ndim == 1 else X_batch
    return svr_pipe.predict(Xb)

def predict_y(x_row: np.ndarray) -> float:
    return float(predict_batch(x_row)[0])

def loss_fun(x: np.ndarray, x0: np.ndarray, y_t: float):
    dist_term = float(np.sum(np.abs((x - x0) / mad)))
    y_hat = float(predict_batch(x)[0])
    return (y_hat - y_t)**2 + LAMBDA_DIST * dist_term, y_hat, dist_term

def propose_directed(x: np.ndarray, y_hat: float, y_t: float, step_frac: float):
    n = x.size
    idx_pool = np.arange(n)

    if immutable_idx:
        mask_mut = np.ones(n, dtype=bool)
        mask_mut[list(immutable_idx)] = False
        idx_pool = idx_pool[mask_mut]
        if idx_pool.size == 0:
            return x.copy(), None

    k = int(min(SENS_K, idx_pool.size))
    cand_idx = rng.choice(idx_pool, size=k, replace=False)

    spans = np.maximum(X_high[cand_idx] - X_low[cand_idx], 1e-12)
    base = EPS_GRAD * spans

    Xp = np.tile(x, (k, 1))
    Xm = np.tile(x, (k, 1))
    rows = np.arange(k)
    Xp[rows, cand_idx] = np.clip(Xp[rows, cand_idx] + base, X_low[cand_idx], X_high[cand_idx])
    Xm[rows, cand_idx] = np.clip(Xm[rows, cand_idx] - base, X_low[cand_idx], X_high[cand_idx])

    y_p = predict_batch(Xp)
    y_m = predict_batch(Xm)
    dy = y_p - y_m

    sign_goal = np.sign(y_t - y_hat)
    dir_j = np.sign(dy) * sign_goal
    valid = dir_j != 0
    if not np.any(valid):
        return x.copy(), None

    cand_idx = cand_idx[valid]
    dir_j = dir_j[valid]
    spans = spans[valid]

    steps = dir_j * step_frac * spans
    Xc = np.tile(x, (cand_idx.size, 1))
    rows = np.arange(cand_idx.size)
    Xc[rows, cand_idx] = np.clip(Xc[rows, cand_idx] + steps, X_low[cand_idx], X_high[cand_idx])

    y_new = predict_batch(Xc)
    gains = np.abs(y_hat - y_t) - np.abs(y_new - y_t)
    best = int(np.argmax(gains))
    return Xc[best].copy(), int(cand_idx[best])

def coordinate_annealing(x0: np.ndarray, y_t: float,
                         max_iters=MAX_ITERS, init_step=INIT_STEP, min_step=MIN_STEP, cooling=COOLING):
    x = x0.copy()
    step, T = init_step, 1.0
    best_x = x.copy()
    best_loss, best_y, best_dist = loss_fun(x, x0, y_t)
    cur_loss, cur_y, cur_dist = best_loss, best_y, best_dist
    no_improve = 0

    for _ in range(max_iters):
        cand, j = propose_directed(x, cur_y, y_t, step)
        if j is None:
            # fallback aleatório
            j = int(rng.integers(0, x.size))
            if j in immutable_idx:
                continue
            span = max(X_high[j] - X_low[j], 1e-9)
            cand = x.copy()
            cand[j] = np.clip(cand[j] + (rng.uniform(-1, 1) * step) * span, X_low[j], X_high[j])

        L_new, y_new, dist_new = loss_fun(cand, x0, y_t)

        if (L_new <= cur_loss) or (rng.random() < math.exp(-(L_new - cur_loss) / max(T, 1e-12))):
            x = cand
            cur_loss, cur_y, cur_dist = L_new, y_new, dist_new
            if cur_loss < best_loss - 1e-12:
                best_loss, best_y, best_dist = cur_loss, cur_y, cur_dist
                best_x = x.copy()
                no_improve = 0
            else:
                no_improve += 1
        else:
            no_improve += 1

        if abs(best_y - y_t) <= TOL_Y:
            break

        if no_improve >= NO_IMPROVE_K:
            j = int(rng.integers(0, x.size))
            if j not in immutable_idx:
                span = max(X_high[j] - X_low[j], 1e-9)
                x[j] = np.clip(x[j] + 0.6 * step * span * rng.choice([-1, 1]), X_low[j], X_high[j])
                cur_loss, cur_y, cur_dist = loss_fun(x, x0, y_t)
            no_improve = 0

        T *= cooling
        step = max(step * cooling, min_step)

    return best_x, best_y, best_loss, best_dist

def random_start(x0: np.ndarray) -> np.ndarray:
    x = x0.copy()
    for j in range(x.size):
        if j in immutable_idx:
            continue
        span = max(X_high[j] - X_low[j], 1e-9)
        x[j] = np.clip(x[j] + rng.normal(0, 0.15 * span), X_low[j], X_high[j])
    return x

def delta_norm(x: np.ndarray, x0: np.ndarray):
    return (x - x0) / mad

def l2_norm_delta_norm(x_cf: np.ndarray, x0: np.ndarray) -> float:
    d = delta_norm(x_cf, x0)
    return float(np.sqrt(np.sum(d * d)))

def diverse_select(candidates, x0, top_k, thr):
    selected, deltas_sel = [], []
    for x_cf, y_cf, L_cf, D_cf in candidates:
        d = delta_norm(x_cf, x0)
        if not deltas_sel:
            selected.append((x_cf, y_cf, L_cf, D_cf)); deltas_sel.append(d)
        else:
            if all(np.linalg.norm(d - ds) >= thr for ds in deltas_sel):
                selected.append((x_cf, y_cf, L_cf, D_cf)); deltas_sel.append(d)
        if len(selected) >= top_k:
            break
    if not selected:
        selected = [candidates[0]]
    return selected

def _run_one_restart(x0, y_t, r):
    x_init = x0 if r == 0 else random_start(x0)
    return coordinate_annealing(x_init, y_t)

def run_restarts_parallel(x0, y_t, n_restarts=N_RESTARTS, n_jobs=N_JOBS_RESTARTS):
    outs = Parallel(n_jobs=n_jobs, prefer="processes", verbose=0)(
        delayed(_run_one_restart)(x0, y_t, r) for r in range(n_restarts)
    )
    return outs

# -------------------- Execução --------------------
all_rows = []

for idx in instances_idx:
    if idx < 0 or idx >= X_all.shape[0]:
        print(f"[CF] Índice fora do range: {idx} — pulando.")
        continue

    x0 = X_all[idx].astype(np.float32).copy()
    y0 = predict_y(x0)
    mg0 = float(np.expm1(y0))
    print(f"[CF] Instância {idx}: y0_log1p={y0:.4f} | y0_mg/m3={mg0:.3f}")

    for t_id, y_t in enumerate(y_targets, start=1):
        outs = run_restarts_parallel(x0, y_t, n_restarts=N_RESTARTS, n_jobs=N_JOBS_RESTARTS)
        candidates = list(outs)
        candidates.sort(key=lambda tup: tup[2])  # loss ascendente

        selected = diverse_select(candidates, x0, TOP_K_SOLUTIONS, DIVERSITY_MIN_L2)

        for k, (x_cf, y_cf, L_cf, D_cf) in enumerate(selected, start=1):
            mgcf, mgt = float(np.expm1(y_cf)), float(np.expm1(y_t))
            print(f"   -> alvo {t_id} | sol {k}: y*={mgt:.3f} | obt={mgcf:.3f} | Δ={mgcf-mg0:+.3f} | loss={L_cf:.4f}")

            delta = (x_cf - x0).astype(float)
            rec = {
                "instance_idx": idx,
                "target_id": t_id,
                "solution_rank": k,
                "target_log1p": float(y_t),
                "y0_log1p": float(y0),
                "ycf_log1p": float(y_cf),
                "y0_mgm3": float(mg0),
                "ycf_mgm3": float(mgcf),
                "target_mgm3": float(mgt),
                "loss": float(L_cf),
                "dist_L1_over_MAD": float(np.sum(np.abs((x_cf - x0) / mad))),
                "delta_l2_over_MAD": float(l2_norm_delta_norm(x_cf, x0)),
            }
            for i, name in enumerate(FEATURES):
                rec[f"Δ_{name}"]   = float(delta[i])
                rec[f"x0_{name}"]  = float(x0[i])
                rec[f"xcf_{name}"] = float(x_cf[i])
            all_rows.append(rec)

            if MAKE_PLOTS:
                # 1) Antes/Alvo/CF
                fig, ax = plt.subplots(figsize=(6, 4))
                ax.bar(["Antes", "Alvo", "CF"], [mg0, mgt, mgcf])
                ax.set_ylabel("Chl-a (mg/m³)")
                ax.set_title(f"Inst {idx} • alvo {t_id} • sol {k}")
                for sp in ("top", "right"): ax.spines[sp].set_visible(False)
                plt.tight_layout()
                plt.savefig(OUT_DIR / f"cf_inst{idx}_t{t_id}_s{k}_pred.png", dpi=PLOT_DPI, bbox_inches="tight")
                plt.close(fig)

                # 2) Top-N mudanças
                abs_delta = np.abs(delta)
                order = np.argsort(abs_delta)[::-1][:TOP_N_CHANGES]
                names = [FEATURES[i] for i in order][::-1]
                vals  = [delta[i] for i in order][::-1]
                fig, ax = plt.subplots(figsize=(7, 0.35 * len(names) + 1.5))
                ax.barh(range(len(names)), vals)
                ax.set_yticks(range(len(names))); ax.set_yticklabels(names)
                ax.set_xlabel("Δ (valor sugerido)")
                ax.set_title(f"Inst {idx} • alvo {t_id} • sol {k} — Top {len(names)} mudanças")
                for sp in ("top", "right"): ax.spines[sp].set_visible(False)
                plt.tight_layout()
                plt.savefig(OUT_DIR / f"cf_inst{idx}_t{t_id}_s{k}_deltas.png", dpi=PLOT_DPI, bbox_inches="tight")
                plt.close(fig)

# -------------------- CSV consolidado --------------------
if all_rows:
    df_cf = pd.DataFrame(all_rows)
    csv_path = OUT_DIR / "counterfactuals_topK.csv"
    df_cf.to_csv(csv_path, index=False)
    print(f"[CF] CSV salvo: {csv_path}")
else:
    print("[CF] Nenhum contrafactual gerado.")
